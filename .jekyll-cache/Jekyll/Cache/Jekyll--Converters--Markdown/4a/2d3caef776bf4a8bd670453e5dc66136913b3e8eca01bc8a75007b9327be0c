I"Æ<p>cross-entropy - expectation value of log(p).</p>

<p>initialization - randn for weights. use 2/sqrt(input size) if using relu. See He. Avoids blow up</p>

<p>epoch - one run through all data</p>

<p>mini-batch - break up data into 1 gpus worth chunks. Worth trying different values to see</p>

<p>momentum - smooths gradients that are oscillating and lets build up</p>

<p>Adam - combined momentum and RMS prop. Works better often? 0.9 for beta1 and 0.999 for beta2 are common parameters.</p>

<p>Hyperparameter search - random points use log scales for some things.</p>

<p>reevalute your hyperparametrs occasionally</p>

<p>batch normalization - adds a normalization and mean subtraction at every hidden layer. makes later neurons less susceptible to earlier changes</p>

<p>tensorflow - variables - placeholder, make sessions, run a trainer</p>

<p>strategy - fit training set, dev set, test set, real world</p>

<p>use better optimizer bigger network if not fitting training</p>

<p>use more dat, rgularize if not</p>

<p>satisficing metric</p>

<p>add weight to realyy important examples</p>

<p>biasÂ  - perforance on triainig set - human level is good benchmark</p>

<p>error analysis - ceiling on performance. Find out how many of some kind of problem are happening to figure out what is worthwhile. Do it manually</p>

<p>reasonably robust to random errors in training set</p>

<p>build first fast, iterate fast</p>

<p>if you need to use a different distro from training set, use the real stuff mostly in your dev and test</p>

<p>Break up into train dev and train-dev. so that you can know if the problem is due to mismatch or due to overfitting</p>

<p>manually try to make training set more like dev set on problem cases. Maybe add noise or find more examples of the error prone thing</p>

<p>Transfer learning</p>

<p>Multi-task learning</p>

<p>end to end - use subproblems if you have data for subproblems</p>

<p>Andâ€¦ I canâ€™t access the two last ones yet. Poo.</p>

:ET