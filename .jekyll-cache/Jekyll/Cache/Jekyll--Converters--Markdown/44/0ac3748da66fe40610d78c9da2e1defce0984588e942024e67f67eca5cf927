I"7<p>Matrices are the only math problem man actually knows how to solve.</p>

<p>Everything else is just sort of bullshitting our way through the best we can.</p>

<p>Multiplying matrices like $Latex AB$ is a notation that letâ€™s us reason about very large complicated objects at a very simple level.
The convention of multiplication of matrices seems kind of arbitrary (rows times columns) but it is simple.</p>

<p>The two main powerful questions about a matrix we can get an answer to are what is a matrices inverse and what are itâ€™s eigenvalues.</p>

<p>Hence the emphasis on linear differential equations. Any problem phrased in these terms is just a matrix problem.</p>

<p>Hereâ€™s the biggest most important point of Greenâ€™s Functions. If the differential operator $Latex L$ is considered to be a matrix, then the Greenâ€™s function $Latex G$ is itâ€™s matrix inverse $Latex L^{-1}$.</p>

<p>Boom.</p>

<!-- more -->

<p>Okay so I havenâ€™t discussed at all what Greenâ€™s functions even are yet. Weâ€™ll get there. To avoid the physical and concrete discussion to go into the abstract feeling one is entirely against my typical policy. But I think this is really important and beautiful. Greenâ€™s functions are used in approximation schemes that have exact and obvious brothers when expressed as matrix equations. The matrix notation removes a large variety of clutter of x,y,z,t variables and integrals that obscures what youâ€™re really trying to say. Itâ€™s good.</p>

<p>A function is a vector. Consider a function f defined in a box $Latex 0</p>

<p>At the same time, we can define very sensible matrices that will take any function and bring it to itâ€™s finite difference, or to itâ€™s finite difference approximation of the Laplacian. (Matrices are linear operations that take vectors to vectors. Derivatives and finite differences are both linear operations. The also take a function and give a function. Hence matrices are a reasonable representation.)</p>

<p>Now here an important subtlety arises. Boundary conditions.
Can I really map a derivative as a new function on the vertices?
Try it out for a 1-d case. Letâ€™s suppose I have a function that I sampled at $Latex x= 0, .5, 1$
$Latex \begin{bmatrix} 4 \ 8 \ 3 \end{bmatrix}$
What is a reasonable choice of the derivative. I claim that really there are only two values.
$Latex \frac{1}{.5}\begin{bmatrix} 4 \ -11 \end{bmatrix}$
Looking at where Iâ€™d want to place them, they seem like they should be defined on the red edges connecting the blue vertices Iâ€™ve chosen, of which there are only two.</p>

<p>My derivative matrix is not square! This kind of sucks for some purposes. How am I going to take a second derivative? Gotta build a whole other matrix. Sucks.</p>

<p>The actual form of the matrix $Latex L$ will totally depend on the boundary conditions. This can confusingly be tacit in the ordinary notation like $Latex L=\nabla ^2$. Are we in a metal box? Sphere? Near a needle? That sort of data totally changes the nodes we pick for discretization and how we form L for the nodes on the boundary even if the matrix elements for the nodes in the interior of the domain basically always look like $Latex \nabla^2$. 
The Greenâ€™s function since it comes directly from L also depends on the boundary conditions. $Latex G$ is not always $Latex \frac{1}{r}$! Thatâ€™s only true for the infinite space Laplacian Greenâ€™s function. This was a big point of confusion for me at some point, which is why Iâ€™m emphasizing it now.</p>

<p>Well, that was kind of a blather. Eh, no one will read this anyhow.</p>

<p>I saw a dog once.</p>

<p>It was ok.</p>

:ET